---
title: Word Complexity Prediction Through ML-Based Contextual Analysis
abstract: "This paper conducted a comparative evaluation two approaches  for predicting
  word complexity using \r contextual sentence information, a challenge that traditional
  methods often struggle to address. Two distinct methods \r were explored in this
  work. The first approach combines XLNet word embeddings with a Random Forest classifier
  to processes \r both sentence and word embeddings to predict complexity levels.
  The second approach employs a dual Bidirectional Encoder \r Representations from
  Transformers (BERT) model, consisting of two separate models: one for sentence-level
  complexity and \r another for word-level complexity, with their predictions combined
  for more context-sensitive result. A diverse dataset \r covering the domains of
  religion, biomedical, and parliamentary texts was used, as it is pre-categorised
  into five complexity \r levels (Very-easy, Easy, Medium, Hard, Very-hard). To ensure
  balanced class representation, data augmentation techniques were \r applied. Evaluation
  metrics revealed that the XLNet-based model has performed slightly superior to dual-BERT
  method, achieving \r macro-average F1-score  of 0.79, excelling particularly at
  identifying highly complex words (F1-score = 0.95). In comparison, \r dual-BERT
  achieved a macro-average F1-score equal to 0.78.\r "
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: uzzam25a
month: 0
tex_title: Word Complexity Prediction Through ML-Based Contextual Analysis
firstpage: 53
lastpage: 61
page: 53-61
order: 53
cycles: false
bibtex_author: Uzzam, Muhammad and Htait, Amal
author:
- given: Muhammad
  family: Uzzam
- given: Amal
  family: Htait
date: 2025-08-05
address:
container-title: Proceedings of the UK AI Conference 2024
volume: '295'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 8
  - 5
pdf: https://raw.githubusercontent.com/mlresearch/v295/main/assets/uzzam25a/uzzam25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
